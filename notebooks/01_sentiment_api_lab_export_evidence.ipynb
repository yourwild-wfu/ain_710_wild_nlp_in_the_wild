{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — NLP in the Wild: Sentiment + Confidence Lab (API-based)\n",
    "\n",
    "This notebook analyzes **API-based sentiment classification** results generated by your project.\n",
    "\n",
    "It reads two artifacts from the repo root:\n",
    "- `outputs/sentiment_results.jsonl` (raw per-text results)\n",
    "- `outputs/confidence_summary.json` (derived stability + confidence metrics)\n",
    "\n",
    "It then visualizes:\n",
    "- label distribution\n",
    "- confidence distribution\n",
    "- latency distribution\n",
    "- which texts are flagged for review under your chosen confidence threshold\n",
    "\n",
    "> **Note:** This notebook is **read-only by default** (no API calls). There is an optional cell to re-run a batch if you want to regenerate outputs (costs API calls)."
   ],
   "id": "4a0cae4870cfd12c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    \"\"\"Find repo root by walking upward until we see an 'outputs' directory or '.git'.\"\"\"\n",
    "    p = (start or Path.cwd()).resolve()\n",
    "    for _ in range(10):\n",
    "        if (p / \"outputs\").exists() or (p / \".git\").exists():\n",
    "            return p\n",
    "        p = p.parent\n",
    "    raise RuntimeError(\"Could not find repo root. Run from within the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "OUTPUTS_DIR = REPO_ROOT / \"outputs\"\n",
    "\n",
    "SENTIMENT_RESULTS_PATH = OUTPUTS_DIR / \"sentiment_results.jsonl\"\n",
    "CONFIDENCE_SUMMARY_PATH = OUTPUTS_DIR / \"confidence_summary.json\"\n",
    "\n",
    "REPO_ROOT, SENTIMENT_RESULTS_PATH, CONFIDENCE_SUMMARY_PATH\n",
    "\n",
    "# --- Figure and Table output setup ---\n",
    "EVIDENCE_DIR = OUTPUTS_DIR / \"evidence\"\n",
    "EVIDENCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_current_fig(filename: str):\n",
    "    \"\"\"Save the current matplotlib figure to outputs/evidence.\"\"\"\n",
    "    path = EVIDENCE_DIR / filename\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Saved: {path}\")\n"
   ],
   "id": "8bf483ed7375900c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load sentiment results (JSONL)\n",
    "if not SENTIMENT_RESULTS_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {SENTIMENT_RESULTS_PATH}. Run the batch script first.\")\n",
    "\n",
    "df = pd.read_json(SENTIMENT_RESULTS_PATH, lines=True)\n",
    "df.head()"
   ],
   "id": "1fb6963db7838272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_fixed = df.head()\n",
    "# Define a function to drop columns with only a single unique value\n",
    "def drop_single_unique_value_columns(df, columns):\n",
    "    for col in columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Fix all identified problems\n",
    "single_unique_columns = ['run_id', 'project', 'model', 'run_index', 'logprobs_included']\n",
    "drop_single_unique_value_columns(df_fixed, single_unique_columns)\n",
    "df_fixed"
   ],
   "id": "81c7e214978da364",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_fixed = df.head()\n",
    "def drop_constant_column(df, col_name):\n",
    "    \"\"\"\n",
    "    Drops a column from the DataFrame if it contains only a single unique value.\n",
    "    \"\"\"\n",
    "    if df[col_name].nunique() == 1:\n",
    "        df.drop(columns=[col_name], inplace=True)\n",
    "\n",
    "# Fixing the identified problems with the df_fixed DataFrame.\n",
    "drop_constant_column(df_fixed, 'project')  # Removes column with only 1 unique value.\n",
    "drop_constant_column(df_fixed, 'model')    # Removes column with only 1 unique value.\n",
    "drop_constant_column(df_fixed, 'run_index')  # Removes column with only 1 unique value.\n",
    "drop_constant_column(df_fixed, 'logprobs_included')  # Removes column with only 1 unique value.\n",
    "df_fixed"
   ],
   "id": "edef6f080438fd66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_fixed = df.head()\n",
    "def drop_constant_columns(df, constant_columns):\n",
    "    \"\"\"\n",
    "    Drops columns from the DataFrame that contain only one unique value.\n",
    "    \"\"\"\n",
    "    for col in constant_columns:\n",
    "        if df[col].nunique() == 1:\n",
    "            df.drop(columns=col, inplace=True)\n",
    "\n",
    "# Define the constant columns to be fixed\n",
    "constant_columns = ['project', 'model', 'run_index', 'logprobs_included']\n",
    "\n",
    "# Fix the identified constant columns\n",
    "drop_constant_columns(df_fixed, constant_columns)\n",
    "df_fixed"
   ],
   "id": "8327b72d51ca23cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic sanity checks\n",
    "\n",
    "We expect:\n",
    "- `label` in {positive, neutral, negative}\n",
    "- `confidence` in [0, 1]\n",
    "- `elapsed_ms` positive\n",
    "- stable grouping keys: `text`, `item_index`, `run_index`, `run_id`"
   ],
   "id": "50f830f0c6c0f960"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['label'].value_counts()"
   ],
   "id": "db769ea4d2e9302d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df[['confidence', 'elapsed_ms']].describe()"
   ],
   "id": "e11d1154422985e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize label distribution"
   ],
   "id": "6cba2016b1c09490"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = df['label'].value_counts().plot(kind='bar')\n",
    "ax.set_title('Sentiment label counts')\n",
    "ax.set_xlabel('label')\n",
    "ax.set_ylabel('count')\n",
    "save_current_fig(\"fig_01_label_distribution.png\")\n",
    "plt.show()"
   ],
   "id": "61d142b775aa867a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence distribution\n",
    "\n",
    "This shows how strongly the model *claims* to believe each classification.\n",
    "In your discussion post, you can connect this to the idea that **labels alone hide uncertainty**."
   ],
   "id": "ac2cd42e0ba64501"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = df['confidence'].plot(kind='hist', bins=10)\n",
    "ax.set_title('Confidence distribution')\n",
    "ax.set_xlabel('confidence')\n",
    "ax.set_ylabel('frequency')\n",
    "save_current_fig(\"fig_02_confidence_histogram.png\")\n",
    "plt.show()"
   ],
   "id": "f5bf5272a2ef9fb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency distribution (`elapsed_ms`)\n",
    "\n",
    "This helps you talk about real-world operational constraints: responsiveness, cost, and monitoring."
   ],
   "id": "811814783d94d76"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ax = df['elapsed_ms'].plot(kind='hist', bins=10)\n",
    "ax.set_title('API latency distribution (ms)')\n",
    "ax.set_xlabel('elapsed_ms')\n",
    "ax.set_ylabel('frequency')\n",
    "save_current_fig(\"fig_03_latency_histogram.png\")\n",
    "plt.show()"
   ],
   "id": "f663923199eb14cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-text view (for \"NLP in the Wild\" narrative)\n",
    "\n",
    "This table is easy to screenshot or summarize in your discussion post."
   ],
   "id": "96988f53d8bdc073"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cols = ['text', 'label', 'confidence', 'elapsed_ms', 'run_id', 'item_index', 'run_index']\n",
    "df[cols].sort_values(['item_index','run_index']).reset_index(drop=True)"
   ],
   "id": "1543c0d6a48e49d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Export tables for your discussion post\n",
    "\n",
    "These exports write to `outputs/evidence/` so you can attach tables or copy values without screenshots.\n"
   ],
   "id": "65c0107d5141240"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Export tables to EVIDENCE_DIR ---\n",
    "df[cols].sort_values(['item_index','run_index']).to_csv(EVIDENCE_DIR / \"per_text_results.csv\", index=False)\n",
    "print(f\"Wrote: {EVIDENCE_DIR / 'per_text_results.csv'}\")\n"
   ],
   "id": "65f85ce92741aaf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load derived confidence/stability summary\n",
    "\n",
    "This summary is computed by `src/lab/confidence.py` and is where your governance threshold shows up.\n",
    "\n",
    "If you used `low_conf_threshold=0.70`, mixed/ambiguous texts should be flagged for review."
   ],
   "id": "2be4685b3386ea9d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not CONFIDENCE_SUMMARY_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {CONFIDENCE_SUMMARY_PATH}. Run run_confidence.py first.\")\n",
    "\n",
    "with CONFIDENCE_SUMMARY_PATH.open('r', encoding='utf-8') as f:\n",
    "    summaries = json.load(f)\n",
    "\n",
    "df_sum = pd.DataFrame(summaries)\n",
    "df_sum.sort_values('confidence_mean')\n",
    "df_sum.to_csv(EVIDENCE_DIR / \"confidence_summary.csv\", index=False)\n",
    "print(f\"Wrote: {EVIDENCE_DIR / 'confidence_summary.csv'}\")\n"
   ],
   "id": "bf4199102f324a4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagged for review\n",
    "\n",
    "These are the items where your policy says: **\"Don’t trust the label alone — route to review.\"**"
   ],
   "id": "347c736f292ba59e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_sum[df_sum['low_confidence_flag'] == True].sort_values('confidence_mean')"
   ],
   "id": "80d78b70d829c35f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Re-run batch (costs API calls)\n",
    "\n",
    "Uncomment and run this cell only if you want to regenerate results.\n",
    "\n",
    "Tip: set `n_runs_per_text=5` to make stability/variance more interesting."
   ],
   "id": "ee357bc7cdf7ec2a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# from src.lab.sentiment import build_default_context\n",
    "# from src.lab.sentiment_batch import BatchConfig, run_sentiment_batch\n",
    "#\n",
    "# ctx = build_default_context()\n",
    "#\n",
    "# results = run_sentiment_batch(\n",
    "#     ctx=ctx,\n",
    "#     texts=[\n",
    "#         \"I love the design, but the setup was frustrating.\",\n",
    "#         \"This was a complete waste of time.\",\n",
    "#         \"Absolutely fantastic experience — would recommend.\",\n",
    "#         \"Yeah, great… just what I needed (eye roll).\",\n",
    "#     ],\n",
    "#     config=BatchConfig(n_runs_per_text=5, temperature=0.7, write_results_jsonl=True),\n",
    "# )\n",
    "#\n",
    "# print(f\"Wrote {len(results)} records to: {OUTPUTS_DIR / 'sentiment_results.jsonl'}\")"
   ],
   "id": "88d682d04fce1b62",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion-post hooks\n",
    "\n",
    "- **Label vs confidence:** show that mixed sentiment can be labeled consistently but with lower confidence.\n",
    "- **Operational reality:** latency varies; monitoring and fallbacks matter.\n",
    "- **Governance:** define a threshold (e.g., 0.70) where ambiguous outputs route to human review.\n",
    "- **Bias/insensitivity:** propose test sets with dialect, sarcasm, idioms, and domain language; measure label flips and confidence drops."
   ],
   "id": "aa0d1b363c13c92f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
